{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine translation using LSTM cells on Sequence-to-Sequence models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence-to-Sequence models try to map input sequences to target/output sequences. It is still a prediction problem, but instead of classifying the input to a single or multiple class, the model predicts tokens one by one till the end of the sequence, thereby generating an output sequence.\n",
    "\n",
    "LSTMs are Long-short Term memory networks. They are most commonly used in long sequence problems, as they seem to be good at preserving the history of tokens.\n",
    "\n",
    "Here we will apply LSTM cells in the Sequence-to-Sequence networks on a character level for Machine translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.models import Model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While working on Multiple languages, it is highly recommended to use `utf-8` encoding so that the non-ascii characters are read properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('resource\\en_de.txt', 'r', encoding='utf-8') as f:\n",
    "    data = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of the sentence before the `\\t` is in english and the part after is the translation in German language. Now, we must extract them and store them as input and target values for the ML model to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = []\n",
    "target_texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, line in enumerate(data):\n",
    "    ip,op = line.split('\\t')\n",
    "    input_texts.append(ip)\n",
    "    target_texts.append('\\t' + op + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you could see here, we have deliberately concatenated the `\\t` at the front and `\\n` at the end. These are called START and END tokens. Its more common nowadays to use `<START>` and `<END>` tokens, so its unambiguous. We just use `\\t` and `\\n` for simplicity.\n",
    "\n",
    "Since we will be working on character-level, the `input_chars` and `target_chars` extract the character level vocabulary from the input, target texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_chars = set([l for w in input_texts for l in w])\n",
    "target_chars = set([l for w in target_texts for l in w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-Decoder network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence-to-Sequence adopts the architecture of Encoder-decoder network. In training, the input sequences are fed into an encoder-network, and the target sentences to a decoder-network. In-order to do so, we must initialise the shape of the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to train all the input sequences in the same network, and not all the sequences are of same length, So we must get the maximum length of input and target sequences for the encoder and decoder sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each character token in the sequence is represented by one-hot encoding, thus the length of each token (`num_enc_tokens`, `num_dec_tokens`) is the size of the respective vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_enc_tokens = len(input_chars)\n",
    "num_dec_tokens = len(target_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For one-hot encoding, we must create character-index pairs, so that we can map back and forth. Just make sure that the order of the `input_chars` and `target_chars` are preserved till the end, any change in the list order might lead to improper mapping to their indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_char2idx = dict([(char, i) for i, char in enumerate(input_chars)])\n",
    "target_char2idx = dict([(char, i) for i, char in enumerate(target_chars)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reverse lookup is not needed during training. But, this might come in handy when we do Inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_idx2char = dict([(i, char) for i, char in enumerate(input_chars)])\n",
    "target_idx2char = dict([(i, char) for i, char in enumerate(target_chars)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inorder to fill the encoder - decoder network, the following step instantiates zero matrices with defined shapes for input to encoder input, decoder input and decoder output. The shape of the matrix is the length of the training data, maximum length of the sequence, and the size of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length, num_enc_tokens),dtype='float32')\n",
    "decoder_input_data = np.zeros((len(target_texts), max_decoder_seq_length, num_dec_tokens),dtype='float32')\n",
    "decoder_target_data = np.zeros((len(target_texts), max_decoder_seq_length, num_dec_tokens),dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the empty matrices are created, `encoder_input_data`, `decoder_input_data`, `decoder_ouput_data` have to be filled with data from the english-german training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[idx, t, input_char2idx[char]] = 1.\n",
    "    encoder_input_data[idx, t + 1:, input_char2idx[' ']] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[idx, t, target_char2idx[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[idx, t - 1, target_char2idx[char]] = 1.\n",
    "    decoder_input_data[idx, t + 1:, target_char2idx[' ']] = 1.\n",
    "    decoder_target_data[idx, t:, target_char2idx[' ']] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_units = 128 # The latent units are nothing but the hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None, num_enc_tokens))\n",
    "encoder_lstm = LSTM(latent_units, return_state=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that the activation function in the final layer (`decoder_dense`) is `softmax` activation. Since, it predicts the probability of the next character and it must sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, num_dec_tokens))\n",
    "decoder_lstm = LSTM(latent_units, return_state=True, return_sequences=True)\n",
    "decoder_dense = Dense(num_dec_tokens, activation='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_outputs, _,_ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=64, epochs=50, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/en2de.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_state_h = Input(shape=(latent_units,))\n",
    "input_state_c = Input(shape=(latent_units,))\n",
    "decoder_input_states = [input_state_h, input_state_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_input_states)\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_model = Model([decoder_inputs] + decoder_input_states, [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Run input seq through encoder model\n",
    "    states = encoder_model.predict(input_seq)\n",
    "    \n",
    "    target_seq = np.zeros((1, 1, num_dec_tokens))\n",
    "    target_seq[0, 0, target_char2idx['\\t']] = 1\n",
    "    \n",
    "    end_of_seq = False\n",
    "    decoded_sentence = ''\n",
    "    \n",
    "    while not end_of_seq:\n",
    "        output, h, c = decoder_model.predict([target_seq] + states)\n",
    "        predicted_char_idx = np.argmax(output[0, -1, :])\n",
    "        predicted_char = target_idx2char[predicted_char_idx]\n",
    "        decoded_sentence += predicted_char\n",
    "        \n",
    "        if (predicted_char == '\\n' or len(decoded_sentence) > max_decoder_seq_length):\n",
    "            end_of_seq = True\n",
    "            \n",
    "        target_seq[0, 0, predicted_char_idx] = 1\n",
    "        states = [h, c]\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
